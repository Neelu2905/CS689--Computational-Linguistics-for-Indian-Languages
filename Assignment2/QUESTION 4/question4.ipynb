{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7813500,"sourceType":"datasetVersion","datasetId":4576847},{"sourceId":7815719,"sourceType":"datasetVersion","datasetId":4578613},{"sourceId":7829824,"sourceType":"datasetVersion","datasetId":4588624},{"sourceId":7829836,"sourceType":"datasetVersion","datasetId":4588634},{"sourceId":7829980,"sourceType":"datasetVersion","datasetId":4588718}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from collections import defaultdict\n\ndef calculate_scores(output_file, ground_truth_file, classes):\n    # Read the contents of the files\n    with open(output_file, 'r') as f:\n        output_data = f.read().strip().split('\\n')\n\n    with open(ground_truth_file, 'r') as f:\n        ground_truth_data = f.read().strip().split('\\n')\n\n    # Initialize dictionaries to store TP, FP, and FN for each class\n    tp = defaultdict(int)\n    fp = defaultdict(int)\n    fn = defaultdict(int)\n\n    # Initialize variables to calculate accuracy\n    total_correct = 0\n    total_tokens = 0\n\n    # Iterate through each token in both lists simultaneously\n    for output_line, ground_truth_line in zip(output_data, ground_truth_data):\n        output_tokens = output_line.strip().split()\n        ground_truth_tokens = ground_truth_line.strip().split()\n\n        total_tokens += len(ground_truth_tokens)\n\n        for output_token, ground_truth_token in zip(output_tokens, ground_truth_tokens):\n            if output_token == ground_truth_token:\n                tp[output_token] += 1\n                total_correct += 1\n            else:\n                fp[output_token] += 1\n                fn[ground_truth_token] += 1\n\n    # Calculate precision, recall, and F1-score for each class\n    precision = {}\n    recall = {}\n    f1_score = {}\n    macro_f1_score = 0.0\n\n    for class_label in classes:\n        precision[class_label] = tp[class_label] / (tp[class_label] + fp[class_label]) if (tp[class_label] + fp[class_label]) > 0 else 0\n        recall[class_label] = tp[class_label] / (tp[class_label] + fn[class_label]) if (tp[class_label] + fn[class_label]) > 0 else 0\n        f1_score[class_label] = 2 * (precision[class_label] * recall[class_label]) / (precision[class_label] + recall[class_label]) if (precision[class_label] + recall[class_label]) > 0 else 0\n        macro_f1_score += f1_score[class_label]\n\n    macro_f1_score /= len(classes)\n    accuracy = total_correct / total_tokens\n\n    return precision, recall, f1_score, macro_f1_score, accuracy\n\n# Define the classes\nclasses = ['O', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG', 'B-MISC', 'I-MISC']\n\n# Calculate scores\nprecision, recall, f1_score, macro_f1_score, accuracy = calculate_scores('/kaggle/input/data-chatgpt/output_chatgpt.txt', '/kaggle/input/data-chatgpt/groundTruth.txt', classes)\n\n# Print results\nfor class_label in classes:\n    print(f\"Class: {class_label}\")\n    print(f\"Precision: {precision[class_label]}\")\n    print(f\"Recall: {recall[class_label]}\")\n    print(f\"F1-score: {f1_score[class_label]}\")\n    print()\n\nprint(f\"Macro F1-score: {macro_f1_score}\")\nprint(f\"Accuracy: {accuracy}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-13T03:54:04.792262Z","iopub.execute_input":"2024-03-13T03:54:04.792696Z","iopub.status.idle":"2024-03-13T03:54:04.824251Z","shell.execute_reply.started":"2024-03-13T03:54:04.792665Z","shell.execute_reply":"2024-03-13T03:54:04.822630Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Class: O\nPrecision: 0.9336823734729494\nRecall: 0.9870848708487084\nF1-score: 0.9596412556053812\n\nClass: B-PER\nPrecision: 0.7222222222222222\nRecall: 0.7222222222222222\nF1-score: 0.7222222222222222\n\nClass: I-PER\nPrecision: 0.5882352941176471\nRecall: 0.625\nF1-score: 0.6060606060606061\n\nClass: B-LOC\nPrecision: 0.8571428571428571\nRecall: 0.8571428571428571\nF1-score: 0.8571428571428571\n\nClass: I-LOC\nPrecision: 0\nRecall: 0\nF1-score: 0\n\nClass: B-ORG\nPrecision: 1.0\nRecall: 0.8888888888888888\nF1-score: 0.9411764705882353\n\nClass: I-ORG\nPrecision: 0.8571428571428571\nRecall: 1.0\nF1-score: 0.923076923076923\n\nClass: B-MISC\nPrecision: 0.25\nRecall: 0.045454545454545456\nF1-score: 0.07692307692307693\n\nClass: I-MISC\nPrecision: 0.5\nRecall: 0.15\nF1-score: 0.23076923076923075\n\nMacro F1-score: 0.5907791824876147\nAccuracy: 0.9088098918083463\n","output_type":"stream"}]},{"cell_type":"code","source":"from collections import defaultdict\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score\n\n# Function to read the file and extract NER tags\ndef read_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        lines = file.readlines()\n        data = []\n        for line in lines:\n            line = eval(line.strip())  # Convert string representation to list of tuples\n            tags = [tag[1] for tag in line]\n            data.append(tags)\n        return data\n\n# Function to calculate precision, recall, F1 score, and accuracy\ndef calculate_metrics(ground_truth, output):\n    metrics = defaultdict(lambda: defaultdict(float))\n    accuracy = 0.0\n    total_samples = 0\n\n    for label in ['O', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG', 'B-MISC', 'I-MISC']:\n        y_true = [tag == label for tags in ground_truth for tag in tags]\n        y_pred = [tag == label for tags in output for tag in tags]\n        precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n        metrics[label]['precision'] = precision\n        metrics[label]['recall'] = recall\n        metrics[label]['f1_score'] = f1_score\n    \n    # Calculate accuracy\n    y_true_flat = [tag for tags in ground_truth for tag in tags]\n    y_pred_flat = [tag for tags in output for tag in tags]\n    accuracy = accuracy_score(y_true_flat, y_pred_flat)\n\n    return metrics, accuracy\n\n# Function to print the metrics\ndef print_metrics(metrics, accuracy):\n    for label, values in metrics.items():\n        print(f\"Metrics for {label}:\")\n        print(f\"Precision: {values['precision']:.4f}\")\n        print(f\"Recall: {values['recall']:.4f}\")\n        print(f\"F1 Score: {values['f1_score']:.4f}\")\n        print()\n    print(f\"Accuracy: {accuracy:.4f}\")\n\n# Paths to input files\nground_truth_file = '/kaggle/input/data-bert/gt_BERT.txt'\noutput_file = '/kaggle/input/data-bert/indicBERT_pred.txt'\n\n# Read data from files\nground_truth_data = read_file(ground_truth_file)\noutput_data = read_file(output_file)\n\n# Calculate metrics\nmetrics, accuracy = calculate_metrics(ground_truth_data, output_data)\n\n# Print individual metrics\nprint_metrics(metrics, accuracy)\n\n# Calculate and print macro F1 score\nmacro_f1 = sum(values['f1_score'] for values in metrics.values()) / len(metrics)\nprint(f\"Macro F1 Score: {macro_f1:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-13T03:56:11.042781Z","iopub.execute_input":"2024-03-13T03:56:11.043216Z","iopub.status.idle":"2024-03-13T03:56:11.829502Z","shell.execute_reply.started":"2024-03-13T03:56:11.043185Z","shell.execute_reply":"2024-03-13T03:56:11.827829Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Metrics for O:\nPrecision: 0.9026\nRecall: 1.0000\nF1 Score: 0.9488\n\nMetrics for B-PER:\nPrecision: 0.5882\nRecall: 0.6250\nF1 Score: 0.6061\n\nMetrics for I-PER:\nPrecision: 0.9231\nRecall: 0.7500\nF1 Score: 0.8276\n\nMetrics for B-LOC:\nPrecision: 0.9091\nRecall: 0.7692\nF1 Score: 0.8333\n\nMetrics for I-LOC:\nPrecision: 0.0000\nRecall: 0.0000\nF1 Score: 0.0000\n\nMetrics for B-ORG:\nPrecision: 0.7143\nRecall: 0.5556\nF1 Score: 0.6250\n\nMetrics for I-ORG:\nPrecision: 0.5000\nRecall: 0.3333\nF1 Score: 0.4000\n\nMetrics for B-MISC:\nPrecision: 0.0000\nRecall: 0.0000\nF1 Score: 0.0000\n\nMetrics for I-MISC:\nPrecision: 0.0000\nRecall: 0.0000\nF1 Score: 0.0000\n\nAccuracy: 0.8893\nMacro F1 Score: 0.4712\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"from collections import defaultdict\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score\n\n# Function to read the file and extract NER tags\ndef read_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        lines = file.readlines()\n        data = []\n        for line in lines:\n            line = eval(line.strip())  # Convert string representation to list of tuples\n            tags = [tag[1] for tag in line]\n            data.append(tags)\n        return data\n\n# Function to calculate precision, recall, F1 score, and accuracy\ndef calculate_metrics(ground_truth, output):\n    metrics = defaultdict(lambda: defaultdict(float))\n    accuracy = 0.0\n    total_samples = 0\n\n    for label in ['O', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG', 'B-MISC', 'I-MISC']:\n        y_true = [tag == label for tags in ground_truth for tag in tags]\n        y_pred = [tag == label for tags in output for tag in tags]\n        precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n        metrics[label]['precision'] = precision\n        metrics[label]['recall'] = recall\n        metrics[label]['f1_score'] = f1_score\n    \n    # Calculate accuracy\n    y_true_flat = [tag for tags in ground_truth for tag in tags]\n    y_pred_flat = [tag for tags in output for tag in tags]\n    accuracy = accuracy_score(y_true_flat, y_pred_flat)\n\n    return metrics, accuracy\n\n# Function to print the metrics\ndef print_metrics(metrics, accuracy):\n    for label, values in metrics.items():\n        print(f\"Metrics for {label}:\")\n        print(f\"Precision: {values['precision']:.4f}\")\n        print(f\"Recall: {values['recall']:.4f}\")\n        print(f\"F1 Score: {values['f1_score']:.4f}\")\n        print()\n    print(f\"Accuracy: {accuracy:.4f}\")\n\n# Paths to input files\nground_truth_file = '/kaggle/input/data-ner/gt_NER.txt'\noutput_file = '/kaggle/input/data-ner/indicNER_pred.txt'\n\n# Read data from files\nground_truth_data = read_file(ground_truth_file)\noutput_data = read_file(output_file)\n\n# Calculate metrics\nmetrics, accuracy = calculate_metrics(ground_truth_data, output_data)\n\n# Print individual metrics\nprint_metrics(metrics, accuracy)\n\n# Calculate and print macro F1 score\nmacro_f1 = sum(values['f1_score'] for values in metrics.values()) / len(metrics)\nprint(f\"Macro F1 Score: {macro_f1:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-13T04:23:52.258914Z","iopub.execute_input":"2024-03-13T04:23:52.259636Z","iopub.status.idle":"2024-03-13T04:23:52.332294Z","shell.execute_reply.started":"2024-03-13T04:23:52.259589Z","shell.execute_reply":"2024-03-13T04:23:52.330783Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Metrics for O:\nPrecision: 0.9241\nRecall: 0.9878\nF1 Score: 0.9549\n\nMetrics for B-PER:\nPrecision: 0.7000\nRecall: 0.7778\nF1 Score: 0.7368\n\nMetrics for I-PER:\nPrecision: 0.7000\nRecall: 1.0000\nF1 Score: 0.8235\n\nMetrics for B-LOC:\nPrecision: 0.7333\nRecall: 0.6111\nF1 Score: 0.6667\n\nMetrics for I-LOC:\nPrecision: 0.0000\nRecall: 0.0000\nF1 Score: 0.0000\n\nMetrics for B-ORG:\nPrecision: 0.5455\nRecall: 0.6000\nF1 Score: 0.5714\n\nMetrics for I-ORG:\nPrecision: 1.0000\nRecall: 0.5000\nF1 Score: 0.6667\n\nMetrics for B-MISC:\nPrecision: 0.0000\nRecall: 0.0000\nF1 Score: 0.0000\n\nMetrics for I-MISC:\nPrecision: 0.0000\nRecall: 0.0000\nF1 Score: 0.0000\n\nAccuracy: 0.8978\nMacro F1 Score: 0.4911\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]}]}